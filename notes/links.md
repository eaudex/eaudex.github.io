# Links

## Deep Learning
* http://karpathy.github.io/2016/05/31/rl/
* https://datascience.stackexchange.com/questions/410/choosing-a-learning-rate
* http://ruder.io/optimizing-gradient-descent/
* http://cs231n.github.io/
* http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf
* http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization

## GBM
* http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf
* http://xgboost.readthedocs.io/en/latest/model.html#

## System Scalability
* https://medium.com/@azifali/how-we-scaled-300k-qps-3-5b-requests-a-day-f69a641f1ae2
* http://www.aosabook.org/en/distsys.html
* http://www.aosabook.org/en/nosql.html
* http://highscalability.com/drop-acid-and-think-about-data
* https://codahale.com/you-cant-sacrifice-partition-tolerance/
* https://www.mongodb.com/faq
* https://stackoverflow.com/questions/12133408/extremely-high-qps-dynamodb-vs-mongodb-vs-other-nosql
* http://www.allthingsdistributed.com/2007/12/eventually_consistent.html
* https://redis.io/topics/cluster-tutorial

## Push vs Pull Model
* http://nadeeshanihewage.blogspot.com/2012/04/push-vs-pull-model-communication-data.html

